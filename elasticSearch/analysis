analysis，es分析
https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis.html

1.分析过程是通过分析器来分析的，可以在创建索引时指定分析器或者在搜索时指定分析器，如果在创建时没有指定分词器那么使用的是默认分词器

索引时指定分词器：
curl -XPUT 'localhost:9200/user?pretty' -H 'Content-Type: application/json' -d'
{
  "mappings": {
    "_doc": {
      "properties": {
        "title": {
          "type":     "text",
          "analyzer": "standard"
        }
      }
    }
  }
}
'
在搜索时指定的分析器必须和索引定义是使用的分析器一样，ES寻找分词器的顺序(优先级)：
1) An analyzer specified in the query itself. 在query中明确指定的
2) The search_analyzer mapping parameter. search_analyzer参数映射指定的分词器
3) The analyzer mapping parameter. analyzer映射参数指定的分词器
4) An analyzer in the index settings called default_search. 索引设置中指定的default_search对应的值
5) An analyzer in the index settings called default. 索引设置中default对应的值
6) The standard analyzer. 系统自带的标准分词器

2.anatomy of tokenizer，解剖分词器
https://www.elastic.co/guide/en/elasticsearch/reference/current/analyzer-anatomy.html

无论是自带的分析器还是自定义的分析器，这些分析器都可能由字符过滤(任意个，按顺序应用)、分词器(只有一个)和token过滤(任意个，按顺序应用)三部分组成

3.testing analyzers，测试分析器
https://www.elastic.co/guide/en/elasticsearch/reference/current/_testing_analyzers.html

使用分词器测试api来测试分词器效果是非常有必要的
https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-analyze.html

例如：
➜  ~ curl -XPOST "http://localhost:9200/_analyze?pretty" -H 'Content-Type: application/json' -d'
{
  "tokenizer": "standard",
  "filter":  [ "lowercase", "asciifolding" ],
  "text":      "Is this déja ME 啊哈?"
}'

其中一部分数据是：
{
      "token" : "哈",
      "start_offset" : 17,
      "end_offset" : 18,
      "type" : "<IDEOGRAPHIC>",
      "position" : 5
}
分词器在分词的过程中不仅产生了分词，还生成了position(used for phrase queries or word proximity queries)和start_offset end_offset 
(used for highlighting search snippets)。postion用于单词和相近词搜索，offset用户高亮搜索

自定义分词器需先在索引中定义然后在映射中引用，测试自定义分词器的时候可以通过分词器名称或者通过映射的字段来引用

也可以在测试的时候使用临时自定义分词器来测试
https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-analyze.html

测试一个自定义临时分词器:
➜  ~ curl -XGET 'localhost:9200/_analyze?pretty' -H 'Content-Type: application/json' -d'
{
  "filter" : ["lowercase"],
  "text" : "BaR"
}
'

测试自定义分词器：
➜  ~ curl -XPOST "http://localhost:9200/_analyze?pretty" -H 'Content-Type: application/json' -d'
{
  "tokenizer": {
          "type": "ngram",
          "min_gram": "1",
          "max_gram": "6"
        },
  "text":      "这是测试"
}'

4.分析器
https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-analyzers.html

5.tokenizers，分词器分为内置分词器和自定义分词器，内置分词器可以用于构建自定义分词器
https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenizers.html

面向单词的分词器：
5.1 Standard Tokenizer，特点：使用字符和空格分词，点不会被分割，会保留字母、数字、下划线、点、英文的'
https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-standard-tokenizer.html
标准分词器将标点符号去掉然后分词，分词最大长度为max_token_length所控制默认为255，可以自定义这个值来设置最长分词长度：
POST _analyze
{
  "tokenizer":  {
          "type": "standard",
          "max_token_length": 5
        },
  "text": "The 2 b,as,,eds-on text"
}

一般情况是：
POST _analyze
{
  "tokenizer": "standard",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}

5.2 letter tokenizer
https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-letter-tokenizer.html
字母分词器以非字母进行分割，中文会被归入字母行列:
➜  ~ curl -XPOST "http://localhost:9200/_analyze?pretty" -H 'Content-Type: application/json' -d'
{
  "tokenizer": "letter",
  "text": "The 2 QUICK Brown-Foxes 啊哈哈ss."
}'
上面分词器结果有一段：
{
  "token" : "啊哈哈ss",
  "start_offset" : 24,
  "end_offset" : 29,
  "type" : "word",
  "position" : 4
}
    
5.3 lowercase tokenizer，小写字母分词器是在字母分词器的基础上加了小写过滤
https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lowercase-tokenizer.html

5.4 whitespace tokenzier，空白符分词器是用空白分割单词的
https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-whitespace-tokenizer.html

5.5 uax url email tokenize
https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-uaxurlemail-tokenizer.html
类似于standard分词器，但是保留url，email等

5.6 classic tokenizer，良好的英文文档分词器
https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-classic-tokenizer.html

5.7 thai tokenizer，泰语分词器
https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-thai-tokenizer.html

部分单词分词器
5.8 ngram tokenizer
https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-ngram-tokenizer.html
递进式分割字符串，可以指定最小分割长度(默认为1)，最大分割长度(默认为2)，还可以指定token_chars(默认是[]保留所有)来过滤只分割的内容
例如：等额3个字符长度分割字符串，结果只包含数字和字母，中文一个汉字和英文一个字母是等同的都视为字母
POST _analyze
{
  "tokenizer": {"type": "ngram",
          "min_gram": 3,
          "max_gram": 3,
          "token_chars": [
            "letter",
            "digit"
          ]},
  "text": "2Quick Fox"
}

当最小和最大值相同时等额分割字符串:

{
  "token": " Fo",
  "start_offset": 7,
  "end_offset": 10,
  "type": "word",
  "position": 7
}

5.9 edge n-gram tokenizer
https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-edgengram-tokenizer.html
边缘n-gram分词器，跟n-gram分词不同的是递进规则，n-gram是从左到右一个字符一个字符依次递进，而edge n-gram则是从第一个字符开始固定加后面
字符递进
例如：
POST _analyze
{
  "tokenizer": "edge_ngram",
  "text": "Quick Fox"
}

结果是：
[ Q, Qu ]

edge n-gram适合搜索歌曲或电影名称，也适合完成自动完成建议
https://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters-completion.html

结构化分词器通常用于结构化文本，像email、zip代码、路径等等不是用于全文分词
5.10 keyword tokenizer
https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-keyword-tokenizer.html
无操作分词器

5.11 pattern tokenizer
https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-pattern-tokenizer.html
正则表达式分词器，比如可以使用逗号分隔数据：
➜  ~ curl -XPOST "http://localhost:9200/_analyze?pretty" -H 'Content-Type: application/json' -d'
{
  "tokenizer": {
    "type": "pattern",
    "pattern": ","
},
  "text": "测试,数据, test data"
}'
{
  "tokens" : [
    {
      "token" : "测试",
      "start_offset" : 0,
      "end_offset" : 2,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "数据",
      "start_offset" : 3,
      "end_offset" : 5,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : " test data",
      "start_offset" : 6,
      "end_offset" : 16,
      "type" : "word",
      "position" : 2
    }
  ]
}

5.12 simple pattern tokenizer 
https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-simplepatternsplit-tokenizer.html

5.13 simple pattern split tokenizer
https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-simplepatternsplit-tokenizer.html
上面这三个正则分词器一个比一个要求严格，但是速度越来越快

5.14 path hierarchy tokenizer
https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-pathhierarchy-tokenizer.html
路径层次化分词器一般用于路径处理
例如：
➜  ~ curl -XPOST "http://localhost:9200/_analyze?pretty" -H 'Content-Type: application/json' -d'
{
  "tokenizer": "path_hierarchy",
  "text": "/one/two/three"
}'

结果：
[ /one, /one/two, /one/two/three ]

6.token filters
https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenfilters.html

es有一系列的内置token过滤器，它们可以用来修改、添加和删除token，也可以构建自定义token过滤器





