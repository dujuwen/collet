
https://zhuanlan.zhihu.com/p/20520370

https://segmentfault.com/a/1190000005840672

https://cloud.tencent.com/developer/article/1032918

https://segmentfault.com/a/1190000005840672

http://www.zving.com/c/2017-11-15/269861.shtml

http://monburan.cn/crawler/2017/04/02/Reflections-About-A-New-Way-To-Anti-Crawler-And-Crawler.html

1. 对 Referer 和 User-Agent 进行检查

ZCMS 将检查 http 头中的 Referer 和 User-Agent 项，对于无 Referer、不符合规则的 Referer、不符合规则的 User-Agent 重定向到验证码输入页面，只有输入正确的验证码才可以进行后续浏览。

2. 基于 IP 地址的访问限制

ZCMS 支持基于 IP 地址的访问限制策略，默认同一个 IP 地址每秒打开页面数超过 3 个即会重定向到一个验证码输入页面，只有输入正确的验证码才可以进行后续浏览。

3. 基于 Cookie 的访问限制

ZCMS 支持基于 Cookie 的访问限制策略，会在每天第一次访问时向浏览器写入特定 Cookie，以后每一次访问都要求携带有特定 Cookie，未携带特定 Cookie 或者同一个 IP 地址频繁变化 Cookie 都将重定向到验证码输入界面。

4. 内容 Javascript 混淆

ZCMS 支持对列表页和内容详细页进行 Javascript 混淆，将列表页和内容详细页中的内容由普通的 HTML 代码自动混淆成 Javascript 代码，正常用户浏览时 Javascript 代码会得到执行并渲染成正常的 HTML 内容，但大部分爬虫不能正确执行这些 Javascript 代码。

5. 基于隐藏链接的行为识别

ZCMS 支持在列表页和内容详细页放入隐藏链接和颜色透明链接，这些链接的形式和正常的 URL 非常相似，正常浏览者不会去点击这些链接，但绝大部分爬虫会解析这些链接并尝试抓取，从而被 ZCMS 识别为爬虫。

6. 基于 URL 规则的行为识别

ZCMS 支持基于 URL 规则的行为识别，正常浏览行为在访问网站时除了详细页以外，还会下载大量的图片、JS 文件、CSS 文件等资源文件，而爬虫不会去下载这些资源文件。ZCMS 通过随机改变部分资源的最后修改时间诱导浏览器下载这些资源文件，而没有去下载的则会被识别为爬虫。

7. 基于时间分布的行为识别

人对单个网站的正常浏览行为在时间分布上会有一定规律，通常会在集中的几个时段内访问。如果检测到某个 IP 或者特定 Cookie 在时间上的分布比较均匀且时间跨度很大（例如一天超过 18 小时都有访问记录），则会被识别为爬虫。

8. 伪数据

ZCMS 对于识别测出的爬虫行为，将不重定向验证码也返回错误信息，而是采用自动生成伪数据的方式向爬虫返回正常的 200 状态响应结果，以提高爬虫数据分析的难度。

https://www.08sec.com/reprinted/16384.html
对于每日的电影院票价这一重要数据，源代码中展示的并不是纯粹的数字。而是在页面使用了 font-face 定义了字符集，并通过 unicode 去映射展示。
简单介绍下这种新型的 web-fongt 反爬虫机制：使用 web-font 可以从网络加载字体，因此我们可以自己创建一套字体，设置自定义的字符映射关系表。
例如设置 0xefab 是映射字符 1，0xeba2 是映射字符 2，以此类推。当需要显示字符 1 时，网页的源码只会是 0xefab，被采集的也只会是 0xefab，并不是 1


https://blog.csdn.net/hguisu/article/details/46126249

http://geek.csdn.net/news/detail/200402

常用公开的防爬方法（按技术难度由简单到难）：
1. Nginx 等 Web Server 层面的限流，人机识别。可以用 Nginx 模块 limit_req或者用 Nginx Lua 自己写一点防爬逻辑，加上 Set-Cookie 和 302 / 验证码的手段，过滤掉一些简单的爬虫。
2：前端打点，正常用户和爬虫的请求最大区别在于是否会渲染 HTML、运行网页的 JavaScript。通过前端打点可以过滤掉一些中级爬虫。
3：IP 库，正经的爬虫大部分都跑在服务器上，通过 IP 库中的非人类访问量甄别服务
4：终极爬虫防护的意义在于让你没法获取到正常的数据，所以构造假数据返回给爬虫是很猥琐的一种手段。

http://blog.163.com/_kid/blog/static/3040547620176211396628/


http://baijiahao.baidu.com/s?id=1574100311626574&wfr=spider&for=pc
1.3 HTTP 层

HTTP 协议层有几个有趣的 HTTP 头，它们是制定反爬虫策略的常用数据。

1.3.1 X-Forwarded-For

X-Forwarded-For（XFF）是用来识别通过 HTTP 代理或负载均衡方式连接到 Web 服务器的客户端最原始的 IP 地址的 HTTP 请求头字段。 Squid 缓存代理服务器的开发人员最早引入了这一 HTTP 头字段，并由 IETF 在 HTTP 头字段标准化草案中正式提出。

XFF 头由普通 HTTP 代理服务器添加， 在用户通过普通 HTTP 代理访问网站时， 用户的 IP 地址会被添加到这个头中。 一些新手程序员在写代码时，往往会把这个的 IP 地址当做用户的真实 IP 地址使用，从而被爬虫利用。

1.3.2 Referer

Referer 是浏览器在页面跳转时带入的 HTTP 头，指示用户上一个页面的 URL， 一般来说，网站 90% 以上的流量应该带有 Referer 头， 在一些常见的反爬策略中， 大量的不带 Referer 头的源 IP 请求会触发 "要求输入验证码" 策略。

1.3.3 User-Agent

User-Agent 是一个古老的 HTTP 头，指示用户浏览器的版本、操作系统等基本信息， UserAgent 伪装已经在其他的文章里有过充分的讨论，故本文不再赘述。

